## Make sure you are at part4/Part4-2
cd '/mnt/c/Users/ixsan/Documents/ETH/0Courses ETH/SS2022/CloudComputing/measures/part4/Part4-2'


## 1. Deploy cluster and install memcached on the server

$ bash scripts/deployP4.sh 

******* WAIT FOR OUTPUT ********

## 2. Expose memcahced to the outside worldn and change parameters:

$ mem=`kubectl get nodes | grep memcache-server | awk '{print $1}'`
$ gcloud compute ssh --ssh-key-file ~/.ssh/cloud-computing ubuntu@$mem --zone europe-west3-a

sudo apt update
sudo apt install -y memcached libmemcached-tools
sudo systemctl status memcached
sudo vim /etc/memcached.conf

-> /-m -> i -> *change value=1024* -> ESC -> :wq!
-> Now change the memory limit: 
	parameter -m: -> change it to 1024

-> Replace the localhost address with the internal IP of the memcache-server VM
	parameter -l: localhost address with internal IP of memchace-server VM

-> Specify the number of threads of memcached: 
	parameter -t: number of threads=2

-> Save the file

$ sudo systemctl restart memcached

******* WAIT FOR OUTPUT ********
*** memcached should be running and listening for req on the VMs internal IP on port 11211 **

## 3. Install the augmented version of mcperf on the other 2 VMs: client measure, client agent

$ bash scripts/install_mcperf_a_m.sh

## 4. INSTALL PYTHON3 in memcached

# COPY 3 files .py in memcache server
	$ wget --output-document scheduler.py https://raw.githubusercontent.com/ixsanpe/cca-project/master/part4/Part4-2/scheduler.py?	token=GHSAT0AAAAAABUX24Y3GEUGRZRLJHUDSDVOYUHRXGQ
	$ wget --output-document utility.py https://raw.githubusercontent.com/ixsanpe/cca-project/master/part4/Part4-2/utility.py?	token=GHSAT0AAAAAABUX24Y3QSHA2CJJVCOJAURSYUHRZIA
	$ wget --output-document config.py https://raw.githubusercontent.com/ixsanpe/cca-project/master/part4/Part4-2/config.py?	token=GHSAT0AAAAAABUX24Y3556EUCI74UYAWW4CYUHRZTA

# GET UPDATE memcached name down
gcloud compute scp --recurse '/mnt/c/Users/ixsan/Documents/ETH/0Courses ETH/SS2022/CloudComputing/measures/part4/Part4-2/scheduler.py' ubuntu@memcache-server-ctcl:/home/ubuntu/  --zone europe-west3-a

gcloud compute scp --recurse '/mnt/c/Users/ixsan/Documents/ETH/0Courses ETH/SS2022/CloudComputing/measures/part4/Part4-2/config.py' ubuntu@memcache-server-ctcl:/home/ubuntu/  --zone europe-west3-a

gcloud compute scp --recurse '/mnt/c/Users/ixsan/Documents/ETH/0Courses ETH/SS2022/CloudComputing/measures/part4/Part4-2/utility.py' ubuntu@memcache-server-ctcl:/home/ubuntu/  --zone europe-west3-a

$ sudo apt install python3-pip
$ pip3 install docker
$ pip3 install psutil

$ cat /var/run/memcached/memcached.pid

# change dataset size -> utility.py -> def run_parsec_job(jobname, cpuset, n_threads, simlarge=True) -> simlarge=False (THIS IS THE BIG ONE)

$ sudo python3 scheduler.py PID NUMBER
$ sudo taskset -c 2,3 python3 scheduler.py 11789


## 5. NEW SCRIPT: Connect to client-agent VM

$ cd '/mnt/c/Users/ixsan/Documents/ETH/0Courses ETH/SS2022/CloudComputing/measures/part4/Part4-2'
$ a=`kubectl get nodes | grep client-agent | awk '{print $1}'`
$ bash scripts/commands_ssh.sh $a
$ cd memcache-perf-dynamic 
$ ./mcperf -T 16 -A

## 6. NEW SCRIPT: Connect to client-agent measure VM


---> Checking if it works:
./mcperf -s $ip_mem -a $ip_a --noload -T 16 -C 4 -D 4 -Q 1000 -c 4 -t 5 --scan 5000:120000:5000



############### PART 4.2
ip
$ cd '/mnt/c/Users/ixsan/Documents/ETH/0Courses ETH/SS2022/CloudComputing/measures/part4/Part4-2'
$ m=`kubectl get nodes | grep client-measure | awk '{print $1}'`

$ kubectl get nodes -o wide
$ bash scripts/commands_ssh.sh $m

$ ip_a=XXXXXXXXX
$ ip_mem=XXXXXXXX

---> NOT NECESSARY
$ cd memcachels-perf-dynamic
$ ./mcperf -s $ip_mem --loadonly
$ ./mcperf -s $ip_mem -a $ip_a --noload -T 16 -C 4 -D 4 -Q 1000 -c 4 -t 1800 --qps_interval 10 --qps_min 5000 --qps_max 100000




#################### PART 4.3

$ cd '/mnt/c/Users/ixsan/Documents/ETH/0Courses ETH/SS2022/CloudComputing/measures/part4/Part4-2'
$ m=`kubectl get nodes | grep client-measure | awk '{print $1}'`

$ kubectl get nodes -o wide

$ ip_a=XXXXXXXXX
$ ip_mem=XXXXXXXX

$ ./mcperf -s $ip_mem --loadonly
$ ./mcperf -s $ip_mem -a $ip_a --noload -T 16 -C 4 -D 4 -Q 1000 -c 4 -t 1800 --qps_interval 10 --qps_min 5000 --qps_max 100000 --qps_seed 42

######## COPY FILE FROM VM TO LOCAL:

gcloud compute scp --recurse ubuntu@memcache-server-rq4t:/home/ubuntu/run_log.json '/mnt/c/Users/ixsan' --zone europe-west3-a

#################### PART 4.4.1

$ cd '/mnt/c/Users/ixsan/Documents/ETH/0Courses ETH/SS2022/CloudComputing/measures/part4/Part4-2'
$ m=`kubectl get nodes | grep client-measure | awk '{print $1}'`

$ kubectl get nodes -o wide

$ ip_a=XXXXXXXXX
$ ip_mem=XXXXXXXX

$ ./mcperf -s $ip_mem --loadonly
$ ./mcperf -s $ip_mem -a $ip_a --noload -T 16 -C 4 -D 4 -Q 1000 -c 4 -t 1800 --qps_interval 5 --qps_min 5000 --qps_max 100000 --qps_seed 42


#################### PART 4.4.2: what is the smallest qps interval to keep SLO < 3%?

$ cd '/mnt/c/Users/ixsan/Documents/ETH/0Courses ETH/SS2022/CloudComputing/measures/part4/Part4-2'
$ m=`kubectl get nodes | grep client-measure | awk '{print $1}'`

$ kubectl get nodes -o wide

$ ip_a=XXXXXXXXX
$ ip_mem=XXXXXXXX

$ ./mcperf -s $ip_mem --loadonly
$ ./mcperf -s $ip_mem -a $ip_a --noload -T 16 -C 4 -D 4 -Q 1000 -c 4 -t 1800 --qps_interval X --qps_min 5000 --qps_max 100000 --qps_seed 42


#####################
delete docker containers:

# list all containers:
$ docker ps -a #list all containers stopped and non stopped

# STOP CONTAINERS:

$ docker stop $(docker ps -a -q)
$ docker kill $(docker ps -q)

## STOP IMAGES AND CONTAINERS NOT RUNNING ANYMORE
$ sudo chmod 666 /var/run/docker.sock
$ docker container prune
$ docker rm -f $(docker ps -a -q) #(delete all)
$ docker stop $(docker ps -a -q)

$ docker image prune

############## delete project:

export KOPS_STATE_STORE=gs://cca-eth-2022-group-25-2-isanchez/
export KOPS_FEATURE_FLAGS=AlphaAllowGCE
PROJECT=cca-eth-2022-group-25-2
kops delete cluster part4.k8s.local --yes
gsutil rm -r gs://cca-eth-2022-group-25-2-isanchez/
gsutil mb gs://cca-eth-2022-group-25-2-isanchez/
